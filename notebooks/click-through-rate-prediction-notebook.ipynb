{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Pattern Match](https://pattern-match.com/img/new-logo.png)\n",
    "\n",
    "# **Amazon SageMaker in Practice - Workshop**\n",
    "## **Click-Through Rate Prediction**\n",
    "\n",
    "This lab covers the steps for creating a click-through rate (CTR) prediction pipeline. The source code of the workshop prepared by [Pattern Match](https://pattern-match.com) is available on the [company's Github account](https://github.com/patternmatch/amazon-sagemaker-in-practice). \n",
    "\n",
    "You can reach authors us via the following emails:\n",
    "\n",
    "- [Sebastian Feduniak](mailto:sebastian.feduniak@pattern-match.com)\n",
    "- [Wojciech Gawroński](mailto:wojciech.gawronski@pattern-match.com)\n",
    "- [Paweł Pikuła](mailto:pawel.pikula@pattern-match.com)\n",
    "\n",
    "Today we use the [Criteo Labs](http://labs.criteo.com/) dataset, used for the old [Kaggle competition](https://www.kaggle.com/c/criteo-display-ad-challenge) for the same purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING**: First you need to update `pandas` to 0.23.4 for the `conda_python3` kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "In advertising, the most critical aspect when it comes to revenue is the final click on the ad. It is one of the ways to compensate for ad delivery for the provider. In the industry, an individual view of the specific ad is called an *impression*.\n",
    "\n",
    "To compare different algorithms and heuristics of ad serving, \"clickability\" of the ad is measured and presented in the form of [*click-through rate* metric (CTR)](https://en.wikipedia.org/wiki/Click-through_rate): \n",
    "\n",
    "![CTR formula](https://wikimedia.org/api/rest_v1/media/math/render/svg/24ae7fdf648530de2083f72ab4b4ae2bc0c47d85)\n",
    "\n",
    "If you present randomly sufficient amount of ads to your user base, you get a baseline level of clicks. It is the easiest and simple solution. However, random ads have multiple problems - starting with a lack of relevance, causing distrust and annoyance.\n",
    "\n",
    "**Ad targeting** is a crucial technique for increasing the relevance of the ad presented to the user. Because resources and a customer's attention is limited, the goal is to provide an ad to most interested users. Predicting those potential clicks based on readily available information like device metadata, demographics, past interactions, and environmental factors is a universal machine learning problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps\n",
    "\n",
    "This notebook presents an example problem to predict if a customer clicks on a given advertisement. The steps include:\n",
    "\n",
    "- Prepare your *Amazon SageMaker* notebook.\n",
    "- Download data from the internet into *Amazon SageMaker*.\n",
    "- Investigate and transforming the data for usage inside *Amazon SageMaker* algorithms.\n",
    "- Estimate a model using the *Gradient Boosting* algorithm (`xgboost`).\n",
    "- Leverage hyperparameter optimization for training multiple models with varying hyperparameters in parallel.\n",
    "- Evaluate and compare the effectiveness of the models.\n",
    "- Host the model up to make on-going predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is *Amazon SageMaker*?\n",
    "\n",
    "*Amazon SageMaker* is a fully managed machine learning service. It enables discovery and exploration with use of *Jupyter* notebooks and then allows for very easy industrialization on a production-grade, distributed environment - that can handle and scale to extensive datasets. \n",
    "\n",
    "It provides solutions and algorithms for existing problems, but you can bring your algorithms into service without any problem. Everything mentioned above happens inside your *AWS infrastructure*. That includes secure and isolated *VPC* (*Virtual Private Cloud*), supported by the full power of the platform.\n",
    "\n",
    "[Typical workflow](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-mlconcepts.html) for creating machine learning models:\n",
    "\n",
    "![Machine Learning with Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/images/ml-concepts-10.png)\n",
    "\n",
    "## Note about *Amazon* vs. *AWS* prefix\n",
    "\n",
    "Why *Amazon* and not *AWS*? \n",
    "\n",
    "Some services available in *Amazon Web Services* portfolio are branded by *AWS* itself, and some by Amazon. \n",
    "\n",
    "Everything depends on the origin and team that maintains it - in that case, it originated from the core of the Amazon, and they maintain this service inside the core division."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with *Amazon SageMaker* locally\n",
    "\n",
    "It is possible to fetch *Amazon SageMaker SDK* library via `pip` and use containers provided by *Amazon* locally, and you are free to do it. The reason why and when you should use *Notebook Instance* is when your datasets are far more significant than you want to store locally and they are residing on *S3* - for such cases it is very convenient to have the *Amazon SageMaker* notebooks available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation\n",
    "\n",
    "The primary way for interacting with *Amazon SageMaker* is to use *S3* as storage for input data and output results. \n",
    "\n",
    "For our workshops, we have prepared two buckets. One is a dedicated bucket for each user (see the credentials card you have received at the beginning of the workshop) - you should put the name of that bucket into `output_bucket` variable. That bucket is used for storing output models and transformed and split input datasets.  \n",
    "\n",
    "We have also prepared a shared bucket called `amazon-sagemaker-in-practice-workshop.pattern-match.com` which contains the input dataset inside a path presented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bucket = 'amazon-sagemaker-in-practice-workshop.pattern-match.com'\n",
    "\n",
    "output_bucket = 'YOUR_USER_BUCKET_NAME_GOES_HERE'\n",
    "\n",
    "path = 'criteo-display-ad-challenge'\n",
    "key = 'sample.csv'\n",
    "\n",
    "data_location = 's3://{}/{}/{}'.format(data_bucket, path, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Amazon SageMaker* as a service runs is a specific security context applied via *IAM role*. You have created that role when creating *notebook instance* before we have uploaded this content. \n",
    "\n",
    "Each *notebook* instance provides a *Jupyter* environment with preinstalled libraries and *AWS SDKs*. One of such *SDKs* is *Amazon SageMaker SDK* available from the *Python* environment. With the use of that *SDK* we can check which security context we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a next, we need to import some stuff. It includes *IPython*, *Pandas*, *numpy*, commonly used libraries from *Python's* Standard Library and *Amazon SageMaker* utilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                                    # For matrix operations and numerical processing\n",
    "import pandas as pd                                   # For munging tabular data\n",
    "import matplotlib.pyplot as plt                       # For charts and visualizations\n",
    "\n",
    "from IPython.display import Image                     # For displaying images in the notebook\n",
    "from IPython.display import display                   # For displaying outputs in the notebook\n",
    "\n",
    "from time import gmtime, strftime                     # For labeling SageMaker models, endpoints, etc.\n",
    "\n",
    "import sys                                            # For writing outputs to notebook\n",
    "import math                                           # For ceiling function\n",
    "import json                                           # For parsing hosting outputs\n",
    "import os                                             # For manipulating filepath names\n",
    "\n",
    "import sagemaker                                      # Amazon SageMaker's Python SDK provides helper functions\n",
    "from sagemaker.predictor import csv_serializer        # Converts strings for HTTP POST requests on inference\n",
    "\n",
    "from sagemaker.tuner import IntegerParameter          # Importing HPO elements.\n",
    "from sagemaker.tuner import CategoricalParameter \n",
    "from sagemaker.tuner import ContinuousParameter\n",
    "from sagemaker.tuner import HyperparameterTuner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to investigate the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "\n",
    "The training dataset consists of a portion of Criteo's traffic over a period of 7 days. Each row corresponds to a display ad served by Criteo and the first column indicates whether this ad was clicked or not. The positive (clicked) and negative (non-clicked) examples have both been subsampled (but at different rates) to reduce the dataset size.\n",
    "\n",
    "There are 13 features taking integer values (mostly count features) and 26 categorical features. Authors hashed values of the categorical features onto 32 bits for anonymization purposes. The semantics of these features is undisclosed. Some features may have missing values (represented as a `-1` for integer values and empty string for categorical ones). Order of the rows is chronological.\n",
    "\n",
    "You may ask, why in the first place we are investigating such *obfuscated* dataset. In *ad tech* it is not unusual to deal with anonymized, or pseudonymized data, which are not semantical - mostly due to privacy and security reasons.\n",
    "\n",
    "The test set is similar to the training set but, it corresponds to events on the day following the training period. For that dataset author removed *label* (the first column).\n",
    "\n",
    "Unfortunately, because of that, it is hard to guess for sure which feature means what, but we can infer that based on the distribution - as we can see below. \n",
    "\n",
    "## Format\n",
    "\n",
    "The columns are tab separeted with the following schema:\n",
    "\n",
    "```\n",
    "<label> <integer feature 1> ... <integer feature 13> <categorical feature 1> ... <categorical feature 26>\n",
    "```\n",
    "\n",
    "When a value is missing, the field is just empty. There is no label field in the test set.\n",
    "\n",
    "Sample dataset (`sample.csv`) contains *100 000* random rows which are taken from a training dataset to ease the exploration. \n",
    "\n",
    "## How to load the dataset?\n",
    "\n",
    "Easy, if it is less than 5 GB - as the disk available on our Notebook instance is equal to 5 GB.\n",
    "\n",
    "However, there is no way to increase that. :( \n",
    "\n",
    "It is because of that EBS volume size is fixed at 5GB. As a workaround, you can use the `/tmp` directory for storing large files temporarily. The `/tmp` directory is on the root drive that has around 20GB of free space. However, data stored there cannot be persisted across stopping and restarting of the notebook instance. \n",
    "\n",
    "What if we need more? We need to preprocess the data in another way (e.g., using *AWS Glue*) and store it on *S3* available for *Amazon SageMaker* training machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read a *CSV* correctly we use *Pandas*. We need to be aware that dataset uses tabs as separators and we do not have the header:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_location, header = None, sep = '\\t')\n",
    "\n",
    "pd.set_option('display.max_columns', 500)                       # Make sure we can see all of the columns.\n",
    "pd.set_option('display.max_rows', 20)                           # Keep the output on one page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration\n",
    "\n",
    "Now we would like to explore our data, especially that we do not know anything about the semantics. How can we do that?\n",
    "\n",
    "We can do that by reviewing the histograms, frequency tables, correlation matrix, and scatter matrix. Based on that we can try to infer and *\"sniff\"* the meaning and semantics of the particular features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integer features\n",
    "\n",
    "First 13 features from the dataset are represented as an integer features, let's review them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms for each numeric features:\n",
    "\n",
    "display(data.describe())\n",
    "\n",
    "%matplotlib inline\n",
    "hist = data.hist(bins = 30, sharey = True, figsize = (10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data.corr())\n",
    "pd.plotting.scatter_matrix(data, figsize = (12, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical features\n",
    "\n",
    "Next 26 features from the dataset are represented as an categorical features. Now it's time to review those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency tables for each categorical feature:\n",
    "\n",
    "for column in data.select_dtypes(include = ['object']).columns:\n",
    "    display(pd.crosstab(index = data[column], columns = '% observations', normalize = 'columns'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_feature = data[14]\n",
    "unique_values = data[14].unique()\n",
    "\n",
    "print(\"Number of unique values in 14th feature: {}\\n\".format(len(unique_values)))\n",
    "print(data[14])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for *integer features*, we can push them as-is to the *Amazon SageMaker* algorithms. We cannot do the same thing for *categorical* one.\n",
    "\n",
    "As you can see above, we have many unique values inside the categorical column. They hashed that into a *32-bit number* represented in a hexadecimal format - as a *string*. \n",
    "\n",
    "We need to convert that into a number, and we can leverage *one-hot encoding* for that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot Encoding\n",
    "\n",
    "It is a way of converting categorical data (e.g., type of animal - *dog*, *cat*, *bear*, and so on) into a numerical one, one-hot encoding means that for a row we create `N` additional columns and we put a `1` if that category is applicable for such row.\n",
    "\n",
    "#### Sparse Vectors\n",
    "\n",
    "It is the more efficient way to store data points which are not dense and do not contain all features. It is possible to efficiently compute various operations between those two forms - dense and sparse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem with *one-hot encoding* in this dataset\n",
    "\n",
    "Unfortunately, we cannot use *OHE* as-is for this dataset. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in data.select_dtypes(include=['object']).columns:\n",
    "    size = data.groupby([column]).size()\n",
    "    print(\"Column '{}' - number of categories: {}\".format(column, len(size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in data.select_dtypes(include=['number']).columns:\n",
    "    size = data.groupby([column]).size()\n",
    "    print(\"Column '{}' - number of categories: {}\".format(column, len(size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have too many distinct categories per feature! In the worst case, for an individual feature, we create couple hundred thousands of new columns. Even with the sparse representation it significantly affects memory usage and execution time.  \n",
    "\n",
    "What kind of features are represented by that? Examples of such features are *Device ID*, *User Agent* strings and similar.\n",
    "\n",
    "How to workaround that? We can use *indexing*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in data.select_dtypes(include = ['object']).columns:\n",
    "    print(\"Converting '{}' column to indexed values...\".format(column))\n",
    "    \n",
    "    indexed_column = \"{}_index\".format(column)\n",
    "    \n",
    "    data[indexed_column] = pd.Categorical(data[column])\n",
    "    data[indexed_column] = data[indexed_column].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_feature = data['14_index']\n",
    "unique_values = data['14_index'].unique()\n",
    "\n",
    "print(\"Number of unique values in 14th feature: {}\\n\".format(len(unique_values)))\n",
    "print(data['14_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in data.select_dtypes(include=['object']).columns:\n",
    "    data.drop([ column ], axis = 1, inplace = True)\n",
    "    \n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is another way of representing a categorical feature in *encoded* form. It is not friendly for *Linear Learner* and classical logistic regression, but we use `xgboost` library - which can leverage such a column without any problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finishing Touches\n",
    "\n",
    "Last, but not least - we need to unify the values that are pointing out a missing value `NaN` and `-1`. We use `NaN` everywhere:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all -1 to NaN:\n",
    "\n",
    "for column in data.columns:\n",
    "    data[column] = data[column].replace(-1, np.nan)\n",
    "    \n",
    "testing = data[2]\n",
    "testing_unique_values = data[2].unique()\n",
    "\n",
    "print(\"Number of unique values in 2nd feature: {}\\n\".format(len(testing_unique_values)))\n",
    "print(testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the dataset\n",
    "\n",
    "We need to split the dataset. We decided to randomize the dataset, and split into 70% for training, 20% for validation and 10% for the test.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sort the data then split out first 70%, second 20%, and last 10%:\n",
    "\n",
    "data_len = len(data)\n",
    "sampled_data = data.sample(frac = 1)\n",
    "\n",
    "train_data, validation_data, test_data = np.split(sampled_data, [ int(0.7 * data_len), int(0.9 * data_len) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After splitting, we need to save new training and validation dataset as *CSV* files. After saving, we upload them to the `output_bucket`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('train.sample.csv', index = False, header = False)\n",
    "validation_data.to_csv('validation.sample.csv', index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3client = boto3.Session().resource('s3')\n",
    "\n",
    "train_csv_file = os.path.join(path, 'train/train.csv')\n",
    "validation_csv_file = os.path.join(path, 'validation/validation.csv')\n",
    "\n",
    "s3client.Bucket(output_bucket).Object(train_csv_file).upload_file('train.sample.csv')\n",
    "s3client.Bucket(output_bucket).Object(validation_csv_file).upload_file('validation.sample.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to leverage *Amazon SageMaker* for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "## Preparation\n",
    "\n",
    "As a first step, we need to point which libraries we want to use. We do that by fetching the container name based on the name of the library we want to use. In our case, it is `xgboost`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "container = get_image_uri(boto3.Session().region_name, 'xgboost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to point out where to look for input data. In our case, we use *CSV* files uploaded in the previous section to `output_bucket`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_key = 's3://{}/{}/train/train.csv'.format(output_bucket, path)\n",
    "validation_csv_key = 's3://{}/{}/validation/validation.csv'.format(output_bucket, path)\n",
    "\n",
    "s3_input_train = sagemaker.s3_input(s3_data = train_csv_key, content_type = 'csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data = validation_csv_key, content_type = 'csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differences from usual workflow and frameworks usage\n",
    "\n",
    "Even that *Amazon SageMaker* supports *CSV* files, most of the algorithms work best when you use the optimized `protobuf` `recordIO` format for the training data. \n",
    "\n",
    "Using this format allows you to take advantage of *pipe mode* when training the algorithms that support it. File mode loads all of your data from *Amazon S3* to the training instance volumes. In *pipe mode*, your training job streams data directly from *Amazon S3*. Streaming can provide faster start times for training jobs and better throughput. \n",
    "\n",
    "With this mode, you also reduce the size of the *Amazon EBS* volumes for your training instances. *Pipe mode* needs only enough disk space to store your final model artifacts. File mode needs disk space to store both your final model artifacts and your full training dataset.\n",
    "\n",
    "For our use case - we leverage *CSV* files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count = 1, \n",
    "                                    train_instance_type = 'ml.m4.xlarge',\n",
    "                                    output_path = 's3://{}/{}/output'.format(output_bucket, path),\n",
    "                                    sagemaker_session = sess)\n",
    "\n",
    "xgb.set_hyperparameters(eval_metric = 'logloss',\n",
    "                        objective = 'binary:logistic',\n",
    "                        eta = 0.2,\n",
    "                        max_depth = 10,\n",
    "                        colsample_bytree = 0.7,\n",
    "                        colsample_bylevel = 0.8,\n",
    "                        min_child_weight = 4,\n",
    "                        rate_drop = 0.3,\n",
    "                        num_round = 75,\n",
    "                        gamma = 0.8)\n",
    "\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to create *Amazon SageMaker session* and `xgboost` framework objects.\n",
    "\n",
    "For a single training job, we need to create *Estimator*, where we point the container and *security context*. In this step, we are specifying the instance type and amount of those used for learning. Last, but not least - we need to specify `output_path` and pass the session object.\n",
    "\n",
    "For the created *Estimator* instance we need to specify the `objective`, `eval_metric` and other hyperparameters used for that training session. \n",
    "\n",
    "As the last step, we need to start the training process passing the training and validation datasets. Whole training job takes approximately 1-2 minutes at most for the following setup.\n",
    "\n",
    "## FAQ\n",
    "\n",
    "**Q**: I see a strange error: `ClientError: Hidden file found in the data path! Remove that before training`. What is that?\n",
    "\n",
    "**A**: There is something wrong with your input files, probably you messed up the *S3* path passed into training job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning (HPO)\n",
    "\n",
    "The single job is just one way. We can automate the whole process with use of *hyperparameter tuning*. \n",
    "\n",
    "As in the case of a single training job, we need to create *Estimator* with the specification for an individual job and set up initial and fixed values for *hyperparameters*. However, outside those - we are setting up the ranges in which algorithm automatically tune in, inside the process of the *HPO*.\n",
    "\n",
    "Inside the *HyperparameterTuner* specification we are specifying how many jobs we want to run and how many of them we want to run in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo_sess = sagemaker.Session()\n",
    "\n",
    "hpo_xgb = sagemaker.estimator.Estimator(container,\n",
    "                                        role, \n",
    "                                        train_instance_count = 1, \n",
    "                                        train_instance_type = 'ml.m4.xlarge',\n",
    "                                        output_path = 's3://{}/{}/output_hpo'.format(output_bucket, path),\n",
    "                                        sagemaker_session = hpo_sess)\n",
    "\n",
    "\n",
    "hpo_xgb.set_hyperparameters(eval_metric = 'logloss',\n",
    "                            objective = 'binary:logistic',\n",
    "                            colsample_bytree = 0.7,\n",
    "                            colsample_bylevel = 0.8,\n",
    "                            num_round = 75,\n",
    "                            rate_drop = 0.3,\n",
    "                            gamma = 0.8)\n",
    "\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "                         'eta': ContinuousParameter(0, 1),\n",
    "                         'min_child_weight': ContinuousParameter(1, 10),\n",
    "                         'alpha': ContinuousParameter(0, 2),\n",
    "                         'max_depth': IntegerParameter(1, 10),\n",
    "                        }\n",
    "\n",
    "objective_metric_name = 'validation:logloss'\n",
    "objective_type = 'Minimize'\n",
    "\n",
    "tuner = HyperparameterTuner(hpo_xgb,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            max_jobs = 20,\n",
    "                            max_parallel_jobs = 5,\n",
    "                            objective_type = objective_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing that is different is how we see the progress of that particular type of the job. In the previous case, logs were shipped automatically into a *notebook*. For *HPO*, we need to fetch job status via *Amazon SageMaker SDK*. Unfortunately, it allows fetching the only status - logs are available in *Amazon CloudWatch*.\n",
    "\n",
    "**Beware**, that with current setup whole *HPO* job may take 20-30 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smclient = boto3.client('sagemaker')\n",
    "\n",
    "job_name = tuner.latest_tuning_job.job_name\n",
    "\n",
    "hpo_job = smclient.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName = job_name)\n",
    "hpo_job['HyperParameterTuningJobStatus']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hosting the single model\n",
    "\n",
    "After finishing the training, *Amazon SageMaker* by default saves the model inside *S3* bucket we have specified. Moreover, based on that model we can either download the archive and use inside our source code and services when deploying, or we can leverage the hosting mechanism available in the *Amazon SageMaker* service. \n",
    "\n",
    "## How it works?\n",
    "\n",
    "After you deploy a model into production using *Amazon SageMaker* hosting services, it creates the endpoint with its configuration. \n",
    "\n",
    "Your client applications use `InvokeEndpoint` API to get inferences from the model hosted at the specified endpoint. *Amazon SageMaker* strips all `POST` headers except those supported by the *API*. Service may add additional headers. \n",
    "\n",
    "Does it mean that everyone can call our model? No, calls to `InvokeEndpoint` are authenticated by using *AWS Signature Version 4*. \n",
    "\n",
    "A customer's model containers must respond to requests within 60 seconds. The model itself can have a maximum processing time of 60 seconds before responding to the /invocations. If your model is going to take 50-60 seconds of processing time, the SDK socket timeout should be set to be 70 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor = xgb.deploy(initial_instance_count = 1, instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Beware**, the '!' in the output after hosting model means that it deployed with success."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hosting the best model from HPO\n",
    "\n",
    "Hosting *HPO* model is no different from a single job. *Amazon SageMaker SDK* in very convenient way selects the best model automatically and uses that as a back-end for the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor_hpo = tuner.deploy(initial_instance_count = 1, instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "After training and hosting the best possible model, we would like to evaluate its performance with `test_data` subset prepared when splitting data.\n",
    "\n",
    "As a first step, we need to prepare our hosted predictors to expect `text/csv` payload, which deserializes via *Amazon SageMaker SDK* entity `csv_serializer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.content_type = 'text/csv'\n",
    "xgb_predictor.serializer = csv_serializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor_hpo.content_type = 'text/csv'\n",
    "xgb_predictor_hpo.serializer = csv_serializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a next step, we need to prepare a helper function that split `test_data` into smaller chunks and serialize them before passing it to predictors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(predictor, data, rows = 500):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    \n",
    "    for array in split_array:\n",
    "        predictions = ','.join([predictions, predictor.predict(array).decode('utf-8')])\n",
    "\n",
    "    return np.fromstring(predictions[1:], sep =',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(xgb_predictor, test_data.drop([0], axis=1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo_predictions = predict(xgb_predictor_hpo, test_data.drop([0], axis=1).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final step, we would like to compare how many clicks available in `test_data` subset were predicted correctly for job trained individually and with *HPO* jobs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = ['actuals']\n",
    "cols = ['predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks = np.round(predictions)\n",
    "result = pd.crosstab(index = test_data[0], columns = clicks, rownames = rows, colnames = cols)\n",
    "\n",
    "display(\"Single job results:\")\n",
    "display(result)\n",
    "display(result.apply(lambda r: r/r.sum(), axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo_clicks = np.round(hpo_predictions)\n",
    "result_hpo = pd.crosstab(index = test_data[0], columns = hpo_clicks, rownames = rows, colnames = cols)\n",
    "\n",
    "display(\"HPO job results:\")\n",
    "display(result_hpo)\n",
    "display(result_hpo.apply(lambda r: r/r.sum(), axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may expect, the model trained with the use of *HPO* works better.\n",
    "\n",
    "What is interesting - without any tuning and significant improvements, we were able to be classified in the first 25-30 results of the leaderboard from the old [Kaggle competition](https://www.kaggle.com/c/criteo-display-ad-challenge/leaderboard). Impressive!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean-up\n",
    "\n",
    "To avoid incurring unnecessary charges, use the *AWS Management Console* to delete the resources that you created for this exercise.\n",
    "\n",
    "Open the *Amazon SageMaker* console at and delete the following resources:\n",
    "\n",
    "1. The endpoint - that also deletes the ML compute instance or instances.\n",
    "2. The endpoint configuration.\n",
    "3. The model.\n",
    "4. The notebook instance. You need to stop the instance before deleting it.\n",
    "\n",
    "Keep in mind that *you can not* delete the history of trained individual and hyperparameter optimization jobs, but that do not incur any charges.\n",
    "\n",
    "Open the Amazon S3 console at and delete the bucket that you created for storing model artifacts and the training dataset. Remember, that before deleting you need to empty it, by removing all objects.\n",
    "\n",
    "Open the *IAM* console at and delete the *IAM* role. If you created permission policies, you could delete them, too.\n",
    "\n",
    "Open the *Amazon CloudWatch* console at and delete all of the log groups that have names starting with `/aws/sagemaker`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to *endpoints* you can leverage the *Amazon SageMaker SDK* for that operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(xgb_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(xgb_predictor_hpo.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
